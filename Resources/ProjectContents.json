[
    {
        "name" : "(WIP) VR Training House", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "Currently being developed as part of a 10-week placement (starting the 17th of January), at the Lancashire Constabulary. The goal of this VR experience is to educate officers on how to interact with common digital items found at crime scenes, and grade them based on their actions within the experience."
            },
            {
                "text" : "The project is being developed using Unity 2021.2, C#, and the XR Interaction Toolkit, with support of all common 6DOF VR headsets. The project is primarily focused on three key areas, the intricate digital device interactions, a dynamic Q&A and scoring system, and the VR house environment itself."
            },
            { 
                "heading": "Project Description",
                "text" : "Currently, the project offers several unique interactions with various digital objects, for instance turning a phone / tablet on and off, browsing its contents, and removing a SIM / SD card. ",
                "imageSrc" : "./Images/Tablet.gif",
                "imageAlt" : ""
            },
            { 
                "text" : "Due to the high probability of officers having no VR experience, all interactions must be intuitive and accessible, and as such I have developed a system which allows officers to perform device interactions in multiple easy-to-use ways. By utilising this custom interaction system officers can navigate UI elements for a device's interactions using the joystick, or by using their other hand as a pointer, as well as it also allowing for trigger presses to perform the device's most important interaction."
            },
            { 
                "text" : "Officers are also graded on their performance within the experience, with each device having a set of questions that officers must answer using pre-defined responses. However, as these questions and responses can change in the wider organisation, I have developed a system to dynamically create the UI for each question based on scriptable objects that contain the questions and responses for each digital device. Meaning, these questions and responses can easily be updated without having to change any objects within the scene.",
                "imageSrc" : "./Images/Question Interaction.gif",
                "imageAlt" : ""
            },
            { 
                "text" : "Similarly, this Q&A forms part of a competence scoring system, that not only allows officers to review their performance in VR, but will also produce a text report file, which will feed directly into the ISO accreditation process.",
                "imageSrc" : "./Images/Questions.gif",
                "imageAlt" : "" 
            },
            { 
                "text" : "In the future, the VR house environment itself will be created, with this environment replicating the Lancashire Constabulary's physical training house and featuring the dynamic and randomised placement of key objects and digital devices - encouraging officers to perform a thorough search of the environment. As well as featuring interactions with doors, cupboards, and other household items, to replicate the training house environment as accurately as possible."
            }
        ]
    },


    {
        "name" : "(WIP) Unreal Brawler Project", 
        "content" : [
            { 
                "heading" : "Overview",
                "text" : "This project was created as part of a four-person team using Unreal Engine 4, with my responsibilities primarily being: ",
                "list" : [
                    { "item" : "Developing the AI systems (using behaviour trees and blackboards)" },
                    { "item" : "Developing the backend animation systems" },
                    { "item" : "Developing the UI systems" },
                    { "item" : "Refining the gameplay systems" }
                ]
            },
            {
                "text" : "I was responsible for creating the AI systems, which makes use of an custom AI controller, behaviour trees, blackboards, and well as code written in both C++ and blueprints. ",
                "video" : "https://www.youtube.com/embed/O9Tb0yvup7M?"
            },
            {
                "text" : "I was also responsible for creating the UI and the backend animations systems (e.g. animation controllers), for both the player and AI.",
                "video" : "https://www.youtube.com/embed/Ja5FeJKe57I?"
            },
            {
                "heading" : "AI Systems",
                "text" : "The AI exhibit custom behaviour depending on a set of criteria, with the AI being able to:",
                "list" : [
                    { "item" : "Patrol the environment" },
                    { "item" : "Intercept and flank the player when attacking" },
                    { "item" : "Investigate the player's last known position on sight loss" },
                    { "item" : "Make smart decisions whether to attack the player or objective" },
                    { "item" : "Spread out when attacking objectives to increase believability and challenge" }
                ]
            },
            {
                "heading" : "Animation Systems",
                "text" : "For the character controller, we were heavily inspired by Overgrowth's procedural animation systems (with I initially experimenting using procedural animation early on in the project). With the final animation system I developed making use of: ",
                "list" : [
                    { "item" : "Blueprints (event graph)" },
                    { "item" : "Animation state machines" },
                    { "item" : "1D & 2D blend spaces" }
                ]
            },
            {
                "heading" : "UI Systems",
                "text" : "With the UI I developed for this project including:", 
                "list" : [
                    { "item" : "Player HUD (including the player's and objective's text and health bars)" },
                    { "item" : "Main menu" },
                    { "item" : "Pause menu" },
                    { "item" : "Basic settings & graphics menus" }
                ]
            }
        ]
    },
    

    {
        "name" : "3D Object Manipulation for Mobile AR (BSc Project)", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "This project forms the basis for the research paper I wrote and submitted to an international conference, as well as my BSc dissertation. It is primarily concerned with allowing 3D objects to be manipulated with Six Degrees-Of-Freedom (6DOF) (i.e. allowing for them to be translated and rotated on all 3-axis), in AR. Without sacrificing granular control and without requiring complex touch gestures, with current mobile AR interaction systems having either (or both) of these limitations, or simply failing to offer full 6DOF control."
            },
            {
                "heading" : "Project Description",
                "text" : "I developed a bespoke system to encode and recognise unique touch gestures (as this was developed before the official release of Unity's new input system), with my system supporting tapping, tap and holding, dragging, and pinching for one and two fingers. Each interaction is assigned at least one gesture and categorised by not only by the touch gesture itself, but also the location on which it is performed (on or off the object). This encoding and detection system is also somewhat abstracted, allowing for minor variations of the gesture to be performed, whilst still recognising the correct input.",
                "video" : "https://www.youtube.com/embed/_6X1hycDETM?"
            },
            {
                "text" : "This system is unique in that the user's perspective of the object determines the axes that the user can manipulate the object on, meaning gestures are kept simple, as they can be re-used for the same interaction on multiple axes. "
            },
            {
                "heading" : "Additional Applications",
                "text" : "To design and evaluate this interaction system two other applications were developed. The first recorded users touch gestures when performing a given interaction (e.g. scaling), and outputted as JSON file with data such as the number of fingers, and the position each frame of the fingers. With this being used to help encode and determine the gestures used by the interaction system.",
                "imageSrc" : "./Images/All GES Tasks.gif",
                "imageAlt" : "A gif showing various animations"
            },
            {
                "text" : "The second was used for evaluation, with it being akin to a “Simon says” game, where users were timed when performing an interaction (e.g. moving the object into a square on the screen), when using different interaction methods. For this, I developed a UI based system which translated the touch gestures into UI elements, but still made use of the user's perspective to determine the available axis. This app compared the bespoke interaction system, to the same manipulation of a physical object, and the UI-based approach.",
                "video" : "https://www.youtube.com/embed/K8WMOohHseQ?"
            }
        ]
    },


    {
        "name" : "Portfolio Website", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "This was an exploration into developing a website from scratch using HTML, CSS, and JS, without making use of frameworks such as Bootstrap or JQuery. The most unique aspect about the site is that it utilises two JSON files to store and dynamically create new project cards, and their content, making adding new projects to the site far easier."
            },
            {
                "text" : "The website is currently being hosted using GitHub Pages and is linked to a custom domain \"DanHarris.Online\". Testing before deployment is done using node.js and the \"live-server\" addon."
            },
            { 
                "heading": "Project Description",
                "text" : "I decided to not use any web frameworks or cite builders to build this website as I thought it would be an interesting challenge and allow me to improve my knowledge of HTML, CSS and JS. The website is inspired by several other portfolio sites, with the design and layout also being heavily influenced by that of the Steam Deck website."    
            },
            { 
                "text" : "The most notable aspect to the website is its dynamic loading of project data, with all project cards, and their content being loaded from two JSON files. A JS script I developed parses the JSON files and automatically creates the project cards and its contents (displayed in the modal), with the idea being to make adding new projects to the website a painless process.",
                "video" : "https://www.youtube.com/embed/KwUOzOsLVMQ?"
            },
            {
                "text" : "Whilst it currently is at an acceptable state, I still plan on improving the code as well as iterating upon its design and functionality. For instance, I intended for the dynamic project loading to allow for even more elements and an even more flexible design of the project card / contents. I also plan on creating the functionality for me to upload a text file and for the website automatically parse it and add it to its respective JSON file, making the addition of projects even easier."
            }
        ]
    }
]
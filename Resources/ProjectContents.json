[
    {
        "name" : "(WIP) VR Training House", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "Currently being developed as part of a 10-week placement (starting the 17th of January), at the Lancashire Constabulary. The goal of this VR experience is to educate officers on how to interact with common digital items found at crime scenes, and grade them based on their actions within the experience."
            },
            {
                "text" : "The project is being developed using Unity 2021.2, C#, and the XR Interaction Toolkit, with support of all common 6DOF VR headsets. The project is primarily focused on three key areas, the intricate digital device interactions, a dynamic Q&A and scoring system, and the VR house environment itself."
            },
            { 
                "heading": "Project Description",
                "text" : "Currently, the project offers several unique interactions with various digital objects, for instance turning a phone / tablet on and off, browsing its contents, and removing a SIM / SD card. ",
                "imageSrc" : "./Images/Tablet.gif",
                "imageAlt" : ""
            },
            { 
                "text" : "Due to the high probability of officers having no VR experience, all interactions must be intuitive and accessible, and as such I have developed a system which allows officers to perform device interactions in multiple easy-to-use ways. By utilising this custom interaction system officers can navigate UI elements for a device's interactions using the joystick, or by using their other hand as a pointer, as well as it also allowing for trigger presses to perform the device's most important interaction."
            },
            { 
                "text" : "Officers are also graded on their performance within the experience, with each device having a set of questions that officers must answer using pre-defined responses. However, as these questions and responses can change in the wider organisation, I have developed a system to dynamically create the UI for each question based on scriptable objects that contain the questions and responses for each digital device. Meaning, these questions and responses can easily be updated without having to change any objects within the scene.",
                "imageSrc" : "./Images/Question Interaction.gif",
                "imageAlt" : ""
            },
            { 
                "text" : "Similarly, this Q&A forms part of a competence scoring system, that not only allows officers to review their performance in VR, but will also produce a text report file, which will feed directly into the ISO accreditation process.",
                "imageSrc" : "./Images/Questions.gif",
                "imageAlt" : "" 
            },
            { 
                "text" : "In the future, the VR house environment itself will be created, with this environment replicating the Lancashire Constabulary's physical training house and featuring the dynamic and randomised placement of key objects and digital devices - encouraging officers to perform a thorough search of the environment. As well as featuring interactions with doors, cupboards, and other household items, to replicate the training house environment as accurately as possible."
            }
        ]
    },


    {
        "name" : "(WIP) Unreal Brawler Project", 
        "content" : [
            { 
                "heading" : "Overview",
                "text" : "I created this game in collaboration with two artists/animators and one other programmer, with our intention being to create a polished character controller and demonstrate this in a Norse inspired futuristic setting and using a wave-based defence mode to showcase our AI and combat system."
            },
            {
                "heading" : "Project Description",
                "text" : "I was responsible for creating the AI systems and the UI backend for this project. With the AI having custom behaviour and choosing to either attempting to attack the objective based on certain criteria."
            },
            {
                "text" : "The AI will attack the player if they have line of sight and if x number of AI are not already attacking the player (determined by an AI controller), similarly, those AI will investigate the player's last known position if they lose sight of the player. The AI will also attempt to both intercept and flank the player to try and surround them and will spread out around the objective when attacking to (to avoid clumps on AI), helping to make their behaviour more natural and increasing the challenge for the player.",
                "video" : "https://www.youtube.com/embed/O9Tb0yvup7M?"
            },
            {
                "text" : "The UI for this project included a functioning HUD for the player, displaying their health and the health of the objective, in addition to a main menu, pause menu, and settings menu, alongside a game over screen."
            },
            {
                "text" : "For the character controller we were heavily inspired by Overgrowth (and its procedural animation system), with my focus early on being to implement the animation controller, blendspace, etc. for the main character. I also initially experimented using procedural animation techniques, although we eventually decided on using authored animations (as seen below).",
                "video" : "https://www.youtube.com/embed/Ja5FeJKe57I?"
            }
        ]
    },
    

    {
        "name" : "3D Object Manipulation for Mobile AR (BSc Project)", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "This project forms the basis for the research paper I wrote and submitted to an international conference, as well as my BSc dissertation. It is primarily concerned with allowing 3D objects to be manipulated with Six Degrees-Of-Freedom (6DOF) (i.e. allowing for them to be translated and rotated on all 3-axis), in AR. Without sacrificing granular control and without requiring complex touch gestures, with current mobile AR interaction systems having either (or both) of these limitations, or simply failing to offer full 6DOF control."
            },
            {
                "heading" : "Project Description",
                "text" : "I developed a bespoke system to encode and recognise unique touch gestures (as this was developed before the official release of Unity's new input system), with my system supporting tapping, tap and holding, dragging, and pinching for one and two fingers. Each interaction is assigned at least one gesture and categorised by not only by the touch gesture itself, but also the location on which it is performed (on or off the object). This encoding and detection system is also somewhat abstracted, allowing for minor variations of the gesture to be performed, whilst still recognising the correct input.",
                "video" : "https://www.youtube.com/embed/_6X1hycDETM?"
            },
            {
                "text" : "This system is unique in that the user's perspective of the object determines the axes that the user can manipulate the object on, meaning gestures are kept simple, as they can be re-used for the same interaction on multiple axes. "
            },
            {
                "heading" : "Additional Applications",
                "text" : "To design and evaluate this interaction system two other applications were developed. The first recorded users touch gestures when performing a given interaction (e.g. scaling), and outputted as JSON file with data such as the number of fingers, and the position each frame of the fingers. With this being used to help encode and determine the gestures used by the interaction system.",
                "imageSrc" : "./Images/All GES Tasks.gif",
                "imageAlt" : "A gif showing various animations"
            },
            {
                "text" : "The second was used for evaluation, with it being akin to a “Simon says” game, where users were timed when performing an interaction (e.g. moving the object into a square on the screen), when using different interaction methods. For this, I developed a UI based system which translated the touch gestures into UI elements, but still made use of the user's perspective to determine the available axis. This app compared the bespoke interaction system, to the same manipulation of a physical object, and the UI-based approach.",
                "video" : "https://www.youtube.com/embed/K8WMOohHseQ?"
            }
        ]
    },


    {
        "name" : "Portfolio Website", 
        "content" : [
            { 
                "heading": "Overview",
                "text" : "This was an exploration into developing a website from scratch using HTML, CSS, and JS, without making use of frameworks such as Bootstrap or JQuery. The most unique aspect about the site is that it utilises two JSON files to store and dynamically create new project cards, and their content, making adding new projects to the site far easier."
            },
            {
                "text" : "The website is currently being hosted using GitHub Pages and is linked to a custom domain \"DanHarris.Online\". Testing before deployment is done using node.js and the \"live-server\" addon."
            },
            { 
                "heading": "Project Description",
                "text" : "I decided to not use any web frameworks or cite builders to build this website as I thought it would be an interesting challenge and allow me to improve my knowledge of HTML, CSS and JS. The website is inspired by several other portfolio sites, with the design and layout also being heavily influenced by that of the Steam Deck website."    
            },
            { 
                "text" : "The most notable aspect to the website is its dynamic loading of project data, with all project cards, and their content being loaded from two JSON files. A JS script I developed parses the JSON files and automatically creates the project cards and its contents (displayed in the modal), with the idea being to make adding new projects to the website a painless process.",
                "video" : "https://www.youtube.com/embed/KwUOzOsLVMQ?"
            },
            {
                "text" : "Whilst it currently is at an acceptable state, I still plan on improving the code as well as iterating upon its design and functionality. For instance, I intended for the dynamic project loading to allow for even more elements and an even more flexible design of the project card / contents. I also plan on creating the functionality for me to upload a text file and for the website automatically parse it and add it to its respective JSON file, making the addition of projects even easier."
            }
        ]
    }
]